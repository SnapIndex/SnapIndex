{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 4 embeddings\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "\n",
    "documents: list[str] = [\n",
    "    \"passage: Hello, World!\",\n",
    "    \"query: Hello, World!\",\n",
    "    \"passage: This is an example passage.\",\n",
    "    \"fastembed is supported by and maintained by Qdrant.\"\n",
    "]\n",
    "\n",
    "#embedding_model = TextEmbedding() #\n",
    "#embeddings: list[np.ndarray] = embedding_model.embed(documents) #\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Embedding in batches\n",
    "Small batch (8‚Äì16) ‚Üí Lower memory use, but slower (more runtime calls).\n",
    "Medium batch (32‚Äì128) ‚Üí Often the sweet spot on CPU.\n",
    "Large batch (256+) ‚Üí Can be faster on GPUs or beefy CPUs, but risks OOM (out of memory) errors.\n",
    "\"\"\"\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") # change model here : MiniLM\n",
    "embeddings = list(embedding_model.embed(documents, batch_size=64))\n",
    "print(f\"Got {len(embeddings)} embeddings\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d5df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastembed\n",
      "  Downloading fastembed-0.7.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.20 (from fastembed)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting loguru<0.8.0,>=0.7.2 (from fastembed)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting mmh3<6.0.0,>=4.1.0 (from fastembed)\n",
      "  Downloading mmh3-5.2.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=2.1.0 in ./.venv/lib/python3.13/site-packages (from fastembed) (2.3.3)\n",
      "Collecting onnxruntime>1.20.0 (from fastembed)\n",
      "  Using cached onnxruntime-1.22.1-cp313-cp313-macosx_13_0_universal2.whl.metadata (4.6 kB)\n",
      "Collecting pillow<12.0.0,>=10.3.0 (from fastembed)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed)\n",
      "  Downloading py_rust_stemmers-0.1.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting requests<3.0,>=2.31 (from fastembed)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<1.0,>=0.15 (from fastembed)\n",
      "  Using cached tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm<5.0,>=4.66 (from fastembed)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.20->fastembed)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.20->fastembed)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (25.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub<1.0,>=0.20->fastembed)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.20->fastembed)\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3.0,>=2.31->fastembed)\n",
      "  Using cached charset_normalizer-3.4.3-cp313-cp313-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3.0,>=2.31->fastembed) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3.0,>=2.31->fastembed) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3.0,>=2.31->fastembed) (2025.8.3)\n",
      "Collecting coloredlogs (from onnxruntime>1.20.0->fastembed)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>1.20.0->fastembed)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.13/site-packages (from onnxruntime>1.20.0->fastembed) (6.32.1)\n",
      "Collecting sympy (from onnxruntime>1.20.0->fastembed)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>1.20.0->fastembed)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>1.20.0->fastembed)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading fastembed-0.7.3-py3-none-any.whl (105 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading mmh3-5.2.0-cp313-cp313-macosx_11_0_arm64.whl (40 kB)\n",
      "Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Downloading py_rust_stemmers-0.1.5-cp313-cp313-macosx_11_0_arm64.whl (272 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp313-cp313-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached onnxruntime-1.22.1-cp313-cp313-macosx_13_0_universal2.whl (34.3 MB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: py-rust-stemmers, mpmath, flatbuffers, tqdm, sympy, pyyaml, pillow, mmh3, loguru, humanfriendly, hf-xet, fsspec, filelock, charset_normalizer, requests, coloredlogs, onnxruntime, huggingface-hub, tokenizers, fastembed\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20/20\u001b[0m [fastembed]20\u001b[0m [fastembed]e-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed charset_normalizer-3.4.3 coloredlogs-15.0.1 fastembed-0.7.3 filelock-3.19.1 flatbuffers-25.2.10 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.34.4 humanfriendly-10.0 loguru-0.7.3 mmh3-5.2.0 mpmath-1.3.0 onnxruntime-1.22.1 pillow-11.3.0 py-rust-stemmers-0.1.5 pyyaml-6.0.2 requests-2.32.5 sympy-1.14.0 tokenizers-0.22.0 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\":memory:\")  # Using an in-process Qdrant\n",
    "## CAN BUILD CACHE\n",
    "\n",
    "docs = [\"Qdrant has Langchain integrations\", \"Qdrant also has Llama Index integrations\"]\n",
    "metadata = [\n",
    "    {\"source\": \"Langchain-docs\"},\n",
    "    {\"source\": \"Llama-index-docs\"},\n",
    "]\n",
    "ids = [42, 2]\n",
    "\n",
    "client.add(\n",
    "    collection_name=\"demo_collection\",\n",
    "    documents=docs,\n",
    "    metadata=metadata,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "search_result = client.query(\n",
    "    collection_name=\"demo_collection\",\n",
    "    query_text=\"This is a query document\"\n",
    ")\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f66dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swastikmishra/Code/SnapIndex/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import doc_loader\n",
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3aedf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 1,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post\\nNeural Architecture Search\\nYuxian Gu, Qinghao Hu, Shang Yang, Haocheng Xi, Junyu Chen, Song Han, Han Cai\\nNVIDIA\\nhttps://github.com/NVlabs/Jet-Nemotron\\nAbstract: We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or\\nexceeds the accuracy of leading full-attention models while significantly improving generation throughput.\\nJet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture\\nexploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a\\npre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block\\ndesigns. The pipeline includes four key components: (1) learning optimal full-attention layer placement\\nand elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing\\nhardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy\\nto Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up\\nto 53.6√ógeneration throughput speedup and 6.1√óprefilling speedup. It also achieves higher accuracy on\\nMMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and\\nMoonlight, despite their larger scale with 15B total and 2.2B activated parameters.\\nQwen2.5\\n1.5B\\nHymba-1.5B\\nJet-Nemotron-2B (Ours)\\nRWKV7-2.9B\\nRWKV7-1.5B\\nMamba2-2.7BPythia-2.8B\\nLlama3.2-1B\\nZamba2\\n1.2B\\nGemma3-1B\\nGemma3-4B\\nRecurrentGemma-2B\\nLlama3.2\\n3B\\nSmollm3-3B\\n47x Acceleration\\nMMLU-Pro 5-Shot Accuracy ‚Üë\\nGeneration Throughput (token/s) ‚Üë\\n0 500 1000 1500 2000 2500 3000\\nTraining FLOPs\\n0.5 zFLOPs 5 zFLOPs 50 zFLOPs\\nFrom Pre-trained From Scratch\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45 Qwen3-1.7B\\n-Base\\nGemma3n-E2B\\nGemma2-2B\\nSmollm2\\n1.7B\\nMiniCPM\\n2B\\n21x Acceleration\\nJet-Nemotron-4B (Ours)\\nFigure 1|Comparison Between Jet-Nemotron and State-of-the-Art Efficient Language Models.\\nThe generation throughput is measured on the NVIDIA H100 GPU under a context length of 64K tokens.\\nJet-Nemotron-2B delivers a higher accuracy than Qwen3-1.7B-Base on MMLU-Pro while achieving 47√ó\\nhigher generation throughput. Jet-Nemotron-4B, despite its larger model size, still achieves higher generation\\nthroughput than all full-attention models with less than 2B parameters.\\n1. Introduction\\nThe rapid rise of Language Models (LMs) [1, 2, 3, 4, 5, 6, 7] marks a transformative era in artificial intelligence,\\nwith these models demonstrating exceptional accuracy across a broad range of tasks. However, their efficiency\\nhas become a significant concern due to the substantial computational and memory demands they impose.\\nThis issue is particularly pronounced in long-context generation and reasoning, where the self-attention\\nmechanism [8] incurs a computational complexity ofùëÇ(ùëõ2) and generates a large Key-Value (KV) cache1.\\nTo address this challenge, substantial efforts have been dedicated to designing more efficient LM architectures\\nby developing attention mechanisms with reducedùëÇ(ùëõ) complexity [9, 10, 11, 12, 13, 14]. In parallel, significant\\n1We refer to the standardùëÇ(ùëõ2) attention as full attention, andùëÇ(ùëõ) attention as linear attention.\\nCorresponding author(s): Han Cai (hcai@nvidia.com).\\n¬© 2025 NVIDIA. All rights reserved.\\narXiv:2508.15884v1  [cs.CL]  21 Aug 2025'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 2,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nArchitecture Design of JetLM: PostNAS\\n#1 Full Attention \\nPlacement and Elimination\\n#2 Linear Attention \\nBlock Selection\\n#3 New Attention \\nBlock Design\\n#4 Hardware-Aware \\nArch. Search\\nMamba2\\nGLA\\nGated \\nDeltaNet\\nRWKV\\n‚Ä¶\\nEmbedding\\nLM Head\\nBlock\\nMLP\\nMLP\\nAttn EAttn\\nAttn EAttn\\nPre-Trained Full\\nAttention Model\\n56\\n58\\n60\\n59.3\\n58.5\\n58.7\\n58.5\\n56.1\\nJetLM Baseline +Full Attention Placement +Linear Attention Select. +New Attention Block +Hardware-Aware Arch. Search \\nCommonsense Acc. ‚Üë\\n52\\n56\\n60\\n58.1\\n56.3\\n55.6\\n55.8\\n52.8\\nMMLU Acc. ‚Üë\\n26\\n31\\n36\\n34.9\\n32.8\\n32.3\\n31.2\\n26.5\\nMath Acc. ‚Üë\\n62\\n67\\n72\\n70.4\\n69.9\\n69.3\\n66.6\\n62.6\\nRetrieval Acc. ‚Üë\\n√ó N\\nJet-Nemotron\\nSConv\\nTime-Mixing\\nSConv\\nWq Wk Wv\\nDConv\\nWo\\nWg\\nKernel\\nGenerator\\nWvWk\\nDConv\\nK dim V dim\\nKernel Size\\nWvWvWq\\nHead Num\\nFigure 2|PostNAS Roadmap.Our pipeline starts from a pre-trained full-attention model and keeps the\\nMLP frozen. It then performs a coarse-to-fine search for efficient attention block designs, first determining the\\noptimal placement of full-attention layers, then selecting the best linear attention block or using a new linear\\nattention block, and finally searching for optimal architectural hyperparameters.\\nwork has focused on constructing hybrid models that combine full and linear attention to strike a balance\\nbetween accuracy and efficiency [15, 16, 17]. While these models offer improved efficiency over full-attention\\narchitectures, their accuracy still significantly falls behind state-of-the-art (SOTA) full-attention models,\\nparticularly on challenging benchmarks such as MMLU(-Pro) [18, 19], mathematical reasoning [20, 21, 22],\\nretrieval [23, 24, 25], coding [26, 27, 28], and long-context tasks [29].\\nThis paper introduces Jet-Nemotron, a new family of LMs that matches the accuracy of SOTA full-attention\\nmodels while delivering exceptional efficiency. Figure 1 compares Jet-Nemotron with previous efficient LMs.\\nNotably, Jet-Nemotron-2B achieves higher accuracy on MMLU-Pro than Qwen3-1.7B-Base [5], while offering\\n47√óhigher generation throughput on the NVIDIA H100 GPU under a context length of 64K.\\nJet-Nemotron is built upon Post Neural Architecture Search (PostNAS), a novel neural architecture exploration\\npipeline (Figure 2) that enables the rapid design of efficient model architectures. Unlike the mainstream LM\\narchitecture design approaches, PostNAS begins with a pre-trained full-attention model, from which it inherits\\nthe Multi-Layer Perceptron (MLP) weights and keeps them frozen throughout the process. This strategy\\nsignificantly reduces training costs while still allowing for comprehensive exploration of the attention block.\\nThe pipeline then proceeds through four key steps to systematically search for optimal attention block designs.\\ni) Full Attention Placement and Elimination.Retaining a few full-attention layers within the model\\n[30] is essential for maintaining high accuracy on challenging tasks such as retrieval. However, the optimal\\nplacement of these layers remains unclear. In Section 2.2, we introduce a novel approach that automatically\\nlearns where to use full-attention layers by training a once-for-all super network [31] (Figure 4). The resulting\\nlearned placement significantly outperforms the commonly used uniform placement strategy in terms of\\naccuracy on MMLU (Figure 5, right).\\nii) Linear Attention Block Selection.After finalizing the placement of full-attention layers, we conduct\\nan attention block search to identify the optimal linear attention block (Section 2.3). Thanks to the low\\ntraining cost of our framework, we can systematically evaluate existing linear attention blocks in terms of\\naccuracy across diverse tasks, training efficiency, and inference speed. Importantly, our approach eliminates\\nthe need to rely on small proxy tasks, such as training tiny LMs (e.g., 50M or 150M parameters), ensuring\\nthat the search results directly translate to improvements in final model accuracy. Moreover, as new linear\\nattention blocks are out, our framework can rapidly evaluate them against prior designs and adopt them if\\nthey demonstrate promising results.\\niii) New Attention Block Design.Our framework also facilitates the rapid design of new attention blocks.\\nAdding convolutions is a widely used strategy to enhance the capacity of linear attention [32]. However, prior\\nmethods rely solely on static convolution kernels, lacking the ability to dynamically adapt convolution kernels‚Äô\\nfeature extraction patterns. In Section 2.4, we introduce a new linear attention block, JetBlock (Figure 2,\\n#3). JetBlock uses a kernel generator to produce dynamic causal convolution kernels conditioned on the\\ninput, which are then applied to the value (V) tokens. Additionally, it removes redundant static convolutions\\non the query (Q) and key (K), streamlining the computation. Compared with prior linear attention blocks,\\n2'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 3,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nArchitecture Design of JetLM: PostNAS\\n#1 Full Attention \\nPlacement and Elimination\\n#2 Linear Attention \\nBlock Selection\\n#3 New Attention \\nBlock Design\\n#4 Hardware-Aware \\nArch. Search\\nMamba2\\nGLA\\nGated \\nDeltaNet\\nRWKV\\n‚Ä¶\\nEmbedding\\nLM Head\\nBlock\\nMLP\\nMLP\\nAttn EAttn\\nAttn EAttn\\nPre-Trained Full\\nAttention Model\\n56\\n58\\n60\\n59.3\\n58.5\\n58.7\\n58.5\\n56.1\\nJet-Nemotron Baseline +Full Attention Placement +Linear Attention Select. +New Attention Block +Hardware-Aware Arch. Search \\nCommonsense Acc. ‚Üë\\n52\\n56\\n60\\n58.1\\n56.3\\n55.6\\n55.8\\n52.8\\nMMLU Acc. ‚Üë\\n26\\n31\\n36\\n34.9\\n32.8\\n32.3\\n31.2\\n26.5\\nMath Acc. ‚Üë\\n62\\n67\\n72\\n70.4\\n69.9\\n69.3\\n66.6\\n62.6\\nRetrieval Acc. ‚Üë\\n√ó N\\nJet-Nemotron\\nSConv\\nTime-Mixing\\nSConv\\nWq Wk Wv\\nDConv\\nWo\\nWg\\nKernel\\nGenerator\\nWvWk\\nDConv\\nK dim V dim\\nKernel Size\\nWvWvWq\\nHead Num\\nFigure 3|PostNAS Accuracy Improvement Breakdown.By applying PostNAS to the baseline model,\\nwe achieve significant accuracy improvements across all benchmarks.\\nJetBlock shows improved accuracy with a small overhead (Table 1).\\niv) Hardware-Aware Architecture Search. Last, in Section 2.5, we introduce a hardware-aware\\narchitecture search to identify optimal architectural hyperparameters. Traditionally, the number of parameters\\nhas been used as a proxy for LM efficiency. However, parameter count does not directly correlate with\\ngeneration efficiency on actual hardware. Our hardware-aware search discovers architectural hyperparameters\\nthat deliver similar generation throughput, while using more parameters to achieve better accuracy (Table 2).\\nWe evaluate Jet-Nemotron across a comprehensive suite of benchmarks, including MMLU(-Pro) [18, 19],\\ncommonsense reasoning [33, 34, 35, 36, 37, 38], mathematical reasoning [20, 21, 22, 39], retrieval [23, 24, 25],\\ncoding [26, 27, 28, 40], and long-context tasks [29]. Our Jet-Nemotron-2B model matches or surpasses SOTA\\nfull-attention models, such as Qwen2.5 [4], Qwen3 [5], Gemma3 [41, 42] and Llama3.2 [2], across all benchmarks,\\nwhile achieving significantly higher generation throughput. Furthermore, the throughput gains are even more\\nsubstantial in long-context settings (Figure 6). For example, with a 256K context length, Jet-Nemotron-2B\\ndelivers a 6.14√óprefilling speedup and a 53.6√ódecoding speedup compared to Qwen3-1.7B-Base. We hope\\nthat our efficient LM family (Jet-Nemotron), our new linear attention block (JetBlock), and our architecture\\ndesign pipeline (PostNAS) will benefit the community and accelerate the development and deployment of\\nnext-generation efficient LMs. We summarize our main contributions below:\\n‚Ä¢ We introduce PostNAS, a novel model architecture exploration paradigm for language models. By reusing\\npre-trained LLMs, PostNAS reduces the cost and risk associated with LLM architecture exploration,\\nenabling faster and more efficient innovation in the architecture design of LMs.\\n‚Ä¢ We offer novel insights into the architecture design of efficient LMs, such as the task-specific importance\\nof attention layers and the finding that KV cache size is a more critical factor than parameter count for\\ngeneration throughput.\\n‚Ä¢ We introduce a novel linear attention block, JetBlock, which integrates linear attention with dynamic\\nconvolution and hardware-aware architecture search. It consistently delivers significant accuracy\\nimprovements over previous linear attention blocks while maintaining comparable generation throughput.\\n‚Ä¢ We introduce Jet-Nemotron, a novel hybrid-architecture LM family that achieves superior accuracy\\nacross a wide range of tasks and offers significantly higher generation throughput than prior SOTA\\nfull-attention models (e.g., Qwen2.5, Qwen3, Gemma3, and Llama3.2). With its strong accuracy and\\nexceptional inference efficiency, Jet-Nemotron offers practical benefits for various applications requiring\\nefficient LMs.\\n2. Method\\n2.1. PostNAS Motivation and Roadmap\\nDesigning new language model architectures is challenging and risky due to the high cost of pre-training.\\nMoreover, the significant gap in computational resources and training data makes it difficult for researchers\\noutside of major organizations to match the accuracy of state-of-the-art full-attention models developed by\\nlarge industry players [4, 41, 2]. This disparity hinders innovation in language model architecture design.\\nThis paper proposes an alternative strategy for developing new language model architectures. Rather than\\npre-training models from scratch, we explore novel architectures by building on top of existing full-attention\\nmodels. This approach dramatically reduces both training costs and data requirements.\\n3'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 4,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nBeam\\nSearch\\nPre-Trained Full \\nAttention Model Once-for-All Super Network\\nMLP\\nMLP\\nAttn\\nAttn\\nLM Head\\nEmbedding\\nSpecialized \\nSub Network\\nMLP\\nMLP\\n ‚ùÑ\\nLM Head\\nEmbedding\\n‚ùÑ\\n‚ùÑ\\n‚ùÑ\\nAttn EAttn\\nAttn EAttn\\n\"\\n\"\\nTraining Step 1\\n‚Ä¶\\nTraining Step t\\nMLP\\nMLP\\nEAttn\\nAttn\\nLM Head\\nEmbedding\\n‚ùÑ\\n‚ùÑ\\n‚ùÑ\\n‚ùÑ\\nMLP\\nMLP\\n ‚ùÑ\\nLM Head\\nEmbedding\\n‚ùÑ\\n‚ùÑ\\n‚ùÑ\\nAttn EAttn\\nAttn EAttn\\n \"\\n\"\\n\"\\n\"\\nFigure 4|Learning to Place Full Attention with PostNAS.We train a once-for-all super network and\\nperform beam search to identify the optimal placement of full attention layers.\\nMMLU Acc. ‚ÜëMMLU\\nRetrieval\\nMath\\nAttention Layer Index #Full Attention Layer √ó\\n-1.15\\n-1.25\\n0.20\\n0.15\\n0.17\\n0.14\\n1 28\\n(a) (b)\\n40\\n49\\n58\\n0 2 4 6 8 10 12\\nUniform\\n PostNAS\\nFigure 5 |(a) Layer Placement Search Results on Qwen2.5-1.5B.Each grid cell represents the\\nsearch objective value of the corresponding attention layer; higher values indicate greater importance.(b)\\nComparison Between PostNAS and Uniform Placement.\\nWhile architectures designed within this framework may not yield optimal results when trained from scratch,\\nwe argue that they remain highly valuable. First, as demonstrated in Figure 1, they can deliver immediate\\ngains in efficiency and accuracy over state-of-the-art full-attention models, translating to practical benefits\\nsuch as improved services and reduced operational costs. Second, our framework serves as a rapid testbed for\\narchitectural innovation. If a new design fails to perform well in this setting, it will be unlikely to succeed\\nin full pre-training [43]. This filtering mechanism helps researchers avoid wasting substantial computational\\nresources on unpromising designs.\\nFigure 2 illustrates the roadmap of PostNAS. Starting from a pre-trained full-attention model, it freezes\\nthe MLP weights and explores attention block designs in a coarse-to-fine manner through four key steps:\\nfull attention placement and elimination (Section 2.2), linear attention block selection (Section 2.3), new\\nattention block design (Section 2.4), and hardware-aware architecture search (Section 2.5). Figure 3 shows\\nthe accuracy improvement breakdown from these steps. We observe substantial accuracy improvements across\\nall benchmarks: +5.3 on MMLU, +8.4 on math, +7.8 on retrieval, and +3.2 on commonsense reasoning.\\n2.2. Full Attention Placement and Elimination\\nIncorporating a few full-attention layers has become a common strategy for improving accuracy [30, 16, 44, 17].\\nThe standard approach applies full attention uniformly across a fixed subset of layers, with the remaining\\nlayers using linear attention. However, this uniform strategy is suboptimal, especially in our setting, where we\\nbegin with a pre-trained full-attention model.\\nTo address this, we propose an automatic method for efficiently determining the placement of full-attention\\nlayers. The overall approach is illustrated in Figure 4. We construct a once-for-all super network [45, 31] by\\naugmenting the pre-trained full-attention model with alternative linear attention paths. During training, we\\n4'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 5,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nrandomly sample an active path at each step, forming a subnetwork, which is trained using feature distillation\\nloss [46, 47, 48].\\nAfter training, we perform beam search [49] to determine the optimal placement of full-attention layers\\nunder a given constraint (e.g., two full-attention layers). The search objective is task-dependent: for MMLU,\\nwe select the configuration with the lowest loss on the correct answer (i.e., maximizing‚àíùëôùëúùë†ùë†), while for\\nmathematical and retrieval tasks, we choose the one with the highest accuracy. As shown in Figure 5(b),\\nPostNAS significantly outperforms uniform placement in terms of accuracy.\\nFigure 5(a) presents the search results for Qwen2.5-1.5B. For each layer, we extract the corresponding\\nsubnetwork from the super network by configuring that layer as full attention while setting all remaining\\nlayers to linear attention. We evaluate the accuracy or loss of each subnetwork on a given task and visualize\\nthe results using a heatmap. Our analysis reveals three key findings:\\nKey Finding 1:In the pre-trained full-attention model, not all attention layers contribute equally.\\nFor MMLU, only two layers exhibit critical importance, while for retrieval tasks, just two to three\\nlayers are particularly crucial.\\nKey Finding 2:Different attention layers contribute to different capabilities. Layers that are critical\\nfor MMLU accuracy are not necessarily important for retrieval tasks.\\nKey Finding 3:The pattern of attention importance becomes more intricate for complex tasks like\\nmathematical reasoning. Fortunately, the combined set of top critical layers identified for MMLU and\\nretrieval already encompasses most of the key layers needed for math.\\nIn addition to these key findings, we observe that the search results remain consistent when using different\\nlinear attention operations. In our final experiments, we use GLA [11] in the once-for-all super network\\ntraining for simplicity and slightly improved training throughput.\\n2.3. Linear Attention Block Selection\\nBuilding on the discovered full-attention layer placement, we conduct an attention block search to identify the\\nmost suitable linear attention block for our setup. In our experiments, we evaluate six SOTA linear attention\\nblocks, including RWKV7 [10], RetNet [12], Mamba2 [50], GLA [11], Deltanet [51], and Gated DeltaNet [32].\\nAfter initial efficiency profiling, we observe that RWKV7 exhibits significantly lower training throughput\\ncompared to other linear attention blocks, possibly due to suboptimal kernel implementation. Consequently,\\nwe exclude it from our training experiments. The results, summarized in Table 1, indicate that Gated DeltaNet\\nachieves the best overall accuracy among the evaluated linear attention blocks. This is attributed to the\\ncombination of two factors: (1) the Data-Dependent Gating Mechanism [52], which dynamically controls\\nwhether the model should focus more on the current token or the history state, and (2) the Delta Rule [53],\\nwhich updates the history state with the information increment from the current token, to save the limited\\nstate memory. Therefore, we proceed with Gated DeltaNet in our experiments.\\n2.4. New Attention Block Design\\nWe propose a new linear attention block, JetBlock, designed to enhance the model‚Äôs expressive power by\\nincorporating dynamic convolution [54, 55] into linear attention. Convolution has been shown to be essential\\nfor achieving strong accuracy in many linear attention blocks [32, 56]. However, prior works typically use\\nstatic convolution kernels, which cannot adapt their feature extraction patterns based on the input.\\nTo address this limitation, we introduce a kernel generator module that dynamically produces convolution\\nkernels based on the input features. The overall structure is shown in Figure 2 (#3). This module shares the\\nsame input as the Q/K/V projection layer and begins with a linear reduction layer to improve efficiency, using\\na reduction ratio of 8. A SiLU activation function [57] is applied, followed by a final linear layer that outputs\\n5'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 6,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nAttention Block Data-Depend Delta Throughput ‚Üë Accuracy ‚Üë\\nGating Rule Training Inference MMLU Math Retreival Common.\\nRWKV7 [10] ‚úì ‚úì 123 2,542 - - - -\\nRetNet [12] 269 2,535 53.6 29.9 63.7 58.1\\nMamba2 [50] 273 3,220 51.5 26.0 68.9 57.5\\nGLA [11] ‚úì 265 3,079 55.8 31.2 66.6 58.5\\nDeltanet [51] ‚úì 254 2,955 48.9 27.4 67.9 56.6\\nGated DeltaNet [32] ‚úì ‚úì 247 2,980 55.6 32.3 69.3 58.7\\nJetBlock ‚úì ‚úì 233 2,885 56.3 32.8 69.9 58.5\\n+ Hardware-Aware Search ‚úì ‚úì 227 2,883 58.1 34.9 70.4 59.5\\nTable 1|Accuracy and Efficiency of JetBlock. JetBlock is designed throughLinear Attention Block\\nSelection, New Attention Block Design, andHardware-Aware Searchin PostNAS. It achieves higher\\naccuracy than previous linear attention blocks while maintaining comparable training and inference efficiency.\\nùëëùêæ ùëëùëâ ùëõhead\\nParams\\n(B)\\nCache Size\\n(MB)\\nThroughput\\n(token/s) ‚Üë Retrieval\\nAccuracy ‚Üë Math\\nAccuracy ‚Üë\\n256 288 4 1.62 154 2,969 67.6 31.3\\n192 384 4 1.64 154 2,961 69.3 32.3\\n128 576 4 1.70 154 2,979 69.5 32.5\\n256 144 8 1.66 154 2,986 68.3 32.1\\n192 192 8 1.70 154 2,970 70.6 32.8\\n128 288 8 1.74 154 2,971 69.6 33.2\\n128 192 12 1.78 154 2,959 68.8 32.9\\n96 256 12 1.84 154 2,955 69.6 34.8\\n64 384 12 1.98 154 2,952 70.1 34.2\\nTable 2|Detailed Results of Hardware-Aware Architecture Search.The gray row is the original\\ndesign [32], while the blue row shows the new design produced by our hardware-aware architecture search.\\nthe convolution kernel weights. We adopt Gated DeltaNet for time-mixing, as it performs best compared with\\nother designs as discussed in Section 2.3.\\nWe apply the dynamic convolution kernels to the value (V) tokens, as applying them to the query (Q) or key\\n(K) tokens offers little benefit. Furthermore, we find that static convolutions on Q and K can be removed with\\nnegligible impact on the final model accuracy once dynamic convolution is applied to V. We adopt this design\\nin our final experiments for its slightly improved efficiency. Table 1 compares JetBlock with previous linear\\nattention blocks. It provides better accuracy on math reasoning and retrieval tasks than Gated DeltaNet\\nwhile maintaining similar efficiency.\\n2.5. Hardware-Aware Architecture Search\\nAfter finalizing the macro architecture, specifically the placement of full-attention layers, and selecting\\nthe linear attention block, we perform a hardware-aware architecture search to optimize core architectural\\nhyperparameters, including key/value dimension and the number of attention heads.\\nConventionally, parameter size is the primary efficiency metric used to guide model architecture design.\\nHowever, this approach is suboptimal, as parameter count does not directly correlate with hardware efficiency.\\nWe address this limitation by using the generation throughput as a direct target for selecting architectural\\nhyperparameters. We find that:\\n6'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 7,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nParams Cache Size Throughput MMLU MMLU-Pro BBHType Model (B) (MB) (token/s) ‚Üë Acc. ‚Üë Acc. ‚Üë Acc. ‚Üë\\nQwen2.5-1.5B [4] 1.5 1,792 241 59.5 28.9 44.1\\nQwen3-1.7B-Base [5] 1.7 7,168 61 60.3 37.8 54.2\\nLlama3.2-3B [2] 1.0 7,168 60 54.9 25.0 47.1\\nMiniCPM-2B-128K [58] 2.8 23,040 18 46.0 18.0 36.5\\nMobileLLM-1.5B [59] 1.5 4,320 101 26.0 9.4 27.2\\nSmollm2-1.7B [60] 1.7 12,288 32 48.5 18.3 35.1\\nDeepSeek-V3-Small@1.3T [6] 2.2/15 - - 53.3 - -\\nùëÇ(ùëõ2)\\nMoonlight@1.2T [61] 2.2/15 - - 60.4 28.1 43.2\\nMamba2-2.7B [50] 2.7 80 2,507 25.1 8.6 25.7\\nRWKV7-1.5B [10] 1.5 24 3,050 41.0 13.4 15.9ùëÇ(ùëõ)\\nRec.Gemma-2B [62] 2.0 16 2,355 28.6 12.8 33.3\\nGemma3n-E2B [42] 2.0 768 701 53.9 24.3 45.1\\nHymba-1.5B [44] 1.5 240 180 49.7 17.4 29.8\\nZamba2-1.2B [16] 1.2 6,114 71 43.1 14.2 19.6\\nJet-Nemotron-2B 2.0 154 2,885 60.8 39.0 58.3\\nHybrid\\nJet-Nemotron-4B 4.0 258 1,271 65.2 44.2 65.0\\nTable 3|Results on MMLU(-Pro) and BBH.DeepSeek-V3-Small@1.3T and Moonlight@1.2T are MoE\\nmodels with 2.2B activated and 15B total parameters, trained on 1.3T and 1.2T tokens, respectively.\\nKey Finding 4:KV cache size is the most critical factor influencing long-context and long-generation\\nthroughput. When the KV cache size is constant, models with different parameter counts exhibit\\nsimilar generation throughput (Table 2).\\nThis is because the decoding stage is typically memory-bandwidth-bound rather than compute-bound. In\\nlong-context scenarios, the KV cache often consumes more memory than the model weights. Reducing its size\\ndecreases memory transfer time per decoding step and enables a larger batch size, thereby improving the\\ngeneration throughput.\\nBased on Finding 4, we fix the KV cache size to match the original design and conduct a small-scale grid\\nsearch over the key dimension, value dimension, and number of attention heads. Table 2 summarizes the\\nresults, where all variants use the same linear attention block (i.e., Gated DeltaNet) but have different\\nconfigurations. The blue row represents our final design, while the gray row corresponds to the original design.\\nOur final configuration achieves a generation throughput comparable to the original while incorporating more\\nparameters and improving accuracy. From Table 1, we can see that our hardware-aware search in PostNAS\\nboosts the JetBlock‚Äôs accuracy, while maintaining training and inference throughput.\\n3. Experiments\\n3.1. Setup\\nJet-Nemotron Model Family.We construct two versions of Jet-Nemotron with different parameter sizes:\\nJet-Nemotron-2B and Jet-Nemotron-4B. We use the Retrieval task to guide the placement of full attention\\nlayers and the MMLU task to guide the placement of sliding window attention (SWA) layers. Jet-Nemotron-2B\\nis built upon Qwen2.5-1.5B [4], incorporating two full-attention layers (No. 15 and 20) for retrieval tasks and\\ntwo sliding window attention (SWA) layers (No. 21 and 22) for multiple-choice tasks like MMLU. We find\\nmultiple-choice tasks mainly rely on the pattern-matching property of the softmax operation to route the\\nknowledge of answers to their options. SWA effectively preserves the accuracy on such tasks. The remaining\\nattention layers are replaced with JetBlock. Similarly, Jet-Nemotron-4B is based on Qwen2.5-3B and includes\\nthree full-attention layers (No. 18, 21, 33) and seven SWA layers (No. 6, 17, 20, 22, 23, 26, and 28). We\\nsummarize the final model architectures in Appendix A.1.\\n7'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 8,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nThroughput Accuracy ‚Üë\\nType Model (token/s) ‚Üë Avg. GSM8K MATH MathQA MMLU-Stem GPQA\\nQwen2.5-1.5B [4] 241 38.4 62.4 13.1 34.4 52.7 29.4\\nQwen3-1.7B-Base [5] 61 42.3 62.8 16.7 46.0 50.8 27.9\\nLlama3.2-3B [2] 60 28.8 25.8 8.6 34.2 45.3 30.1\\nMiniCPM-2B-128K [58] 18 27.6 39.2 5.9 28.5 36.3 28.1\\nùëÇ(ùëõ2)\\nSmollm2-1.7B [60] 32 28.9 30.3 9.2 33.7 41.3 30.1\\nMamba2-2.7B [50] 2,507 16.6 3.0 3.9 24.3 26.6 25.3\\nRWKV7-1.5B [10] 2,669 18.3 5.6 0.8 27.2 34.9 23.0ùëÇ(ùëõ)\\nRec.Gemma-2B [62] 2,355 20.8 13.9 7.6 25.3 28.5 28.6\\nGemma3n-E2B [42] 701 28.3 24.9 10.1 31.1 45.7 31.8\\nHymba-1.5B [44] 180 23.1 17.9 0.8 28.0 40.9 27.9\\nZamba2-1.2B [16] 71 24.8 28.1 5.9 26.0 36.5 27.7\\nJet-Nemotron-2B 2,885 49.6 76.2 23.3 53.8 62.7 32.1\\nHybrid\\nJet-Nemotron-4B 1,271 51.3 78.7 25.2 52.5 65.6 34.6\\nTable 4|Results on Math Tasks.\\nTraining Details.The training consists of two stages. In the first stage, we freeze the MLPs and train the\\nmodel using a distillation loss. In the second stage, we perform full-model training. At the first stage, we use\\na combination of Nemotron-CC [63] and Redstone-QA [64] as our pre-training corpus and train Jet-Nemotron\\nmodels for 50B tokens. This is also the setting in Section 2 where we perform PostNAS. At the second stage,\\nwe include more high-quality data from math [65] and coding [66, 67] domains into our data mixture. The\\nmodels are then trained on 350B tokens. We summarize the experimental costs in Appendix A.2.\\nEvaluationDetails. WeevaluateJet-Nemotronacrossmainstreambenchmarksettings: MMLU(-Pro)[ 18,19],\\nmathematical reasoning [18, 20, 21, 22], commonsense reasoning [33, 34, 35, 36, 37, 38], retrieval [23, 24, 25],\\ncoding [26, 27, 28, 40], and long-context tasks [29]. We compare our models against state-of-the-art full-\\nattention models [2, 4, 5], linear attention models [10, 50], and hybrid models [41, 44]. We adopt 4-shot\\nevaluation for GSM8K [22] and MATH [18] and 5-shot evaluation for GPQA [20] and MMLU-Pro [19]. We\\nuse the official implementation of EvalPlus [40] and CRUXEval [28] for coding tasks. For all other tasks, we\\nuse the zero-shot setting. All evaluations are based on LM-Evaluation-Harness [68].\\nThroughput Testbed. Our throughput evaluation was performed on a DGX H100 server, featuring 8\\nNVIDIA H100 GPUs, 2 Intel Xeon Platinum 8480C (112 cores) CPUs, and 2TB of RAM. For fair and\\nconsistent comparisons, we employ the latest available software versions. Specifically, our environment include\\nPytorch 2.7.0 and Triton 3.3.0. We implement the full-attention block with FlashAttention 2.7.4 [69] and\\nlinear attention blocks with Flash-Linear-Attention 0.2.1 [70]. Model inference is based on the Transformers\\n4.52.0 implementation [71]. The context length is 64K, except stated explicitly, and each model is tested on a\\nsingle H100 GPU. We report the cache sizes for a 64K input context in Table 3. When testing the throughput,\\nwe adopt chunk-prefilling [72] and search for the chunk sizes to maximize the batch size for each model under\\nthe constraint of the GPU memory. In this way, we measure the highest achievable decoding throughput on\\nthe device. We list the batch sizes used for each model in Appendix A.3.\\n3.2. Main Results on Accuracy\\nResults on MMLU(-Pro) and BBH.Table 3 compares Jet-Nemotron with the most advanced efficient\\nlanguage models. Jet-Nemotron-2B achieves 47√óhigher throughput and has 47√ósmaller cache size than\\nQwen3-1.7B-Base, while delivering significantly better accuracy on MMLU, MMLU-Pro, and BBH. Jet-\\nNemotron-2B even outperforms recent MoE models like DeepSeek-V3-Small [6] and Moonlight [61] with\\nlarger activated parameters (2.2B) and much larger total parameters (15B). When scaled to 4B parameters,\\nJet-Nemotron-4B still maintains a 21√óthroughput advantage against Qwen3-1.7B-Base. Compared to other\\nlinear attention and hybrid models, Jet-Nemotron also achieves substantially higher accuracy.\\nResults on Math Tasks.Table 4 reports our results on math tasks. Jet-Nemotron-2B achieves an average\\n8'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 9,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nThroughput Accuracy ‚Üë\\nModel (token/s) ‚Üë Avg. ARC-c ARC-e PIQA Wino. OBQA BoolQ TruthQA\\nQwen2.5-1.5B [4] 241 59.4 45.4 71.2 75.8 63.8 40.2 72.8 46.6\\nQwen3-1.7B-Base [5] 61 60.0 44.9 68.6 75.5 63.8 39.0 79.0 48.8\\nLlama3.2-3B [2] 60 59.9 46.6 72.0 78.0 69.3 40.4 73.9 39.3\\nMiniCPM-2B-128K [58] 18 57.6 41.0 69.4 75.5 63.8 40.6 74.7 38.3\\nSmollm2-1.7B [60] 32 59.7 47.0 73.3 77.7 66.2 44.6 72.5 36.7\\nMamba2-2.7B [50] 2,507 57.2 42.1 70.5 76.1 62.7 41.4 71.5 36.1\\nRWKV7-1.5B [10] 3,050 59.7 46.3 75.7 77.4 67.6 45.4 70.5 34.7\\nRec.Gemma-2B [62] 2,355 46.5 29.4 41.5 66.6 54.1 27.0 72.0 34.7\\nGemma3n-E2B [42] 701 58.6 43.2 73.1 77.0 60.8 40.8 76.0 39.1\\nHymba-1.5B [44] 180 61.2 46.9 76.9 77.7 66.2 41.0 80.8 39.0\\nZamba2-1.2B [16] 71 58.0 44.4 66.8 77.4 65.6 42.8 70.8 38.5\\nJet-Nemotron-2B 2,885 62.0 48.6 74.8 75.4 65.8 40.6 81.2 47.8\\nJet-Nemotron-4B 1,271 64.7 51.7 79.2 78.1 70.5 43.6 83.0 46.6\\nTable 5|Results on Commonsense Tasks.\\nType Model Throughput Accuracy ‚Üë\\n(token/s) ‚Üë Avg. FDA SWDE Squad\\nQwen2.5-1.5B [4] 241 72.4 82.8 86.3 48.1\\nQwen3-1.7B-Base [5] 61 76.1 81.8 89.2 57.2\\nLlama3.2-3B [2] 60 71.3 82.3 89.6 56.4\\nMiniCPM-2B-128K [58] 18 72.6 72.3 86.4 59.1\\nùëÇ(ùëõ2)\\nSmollm2-1.7B [60] 32 68.9 78.1 82.4 46.3\\nMamba2-2.7B [50] 2,507 57.0 51.7 74.3 45.1\\nRWKV7-1.5B [10] 3,050 58.6 54.5 73.3 48.0ùëÇ(ùëõ)\\nRec.Gemma-2.6B [62] 2,355 68.8 62.3 86.4 57.8\\nGemma3n-E2B [73] 701 74.0 77.3 86.4 58.2\\nHymba-1.5B [44] 180 57.1 46.6 74.4 50.2\\nZamba2-1.2B [16] 71 66.4 73.8 80.7 44.8\\nJet-Nemotron-2B 2,885 74.2 80.4 85.7 56.6\\nHybrid\\nJet-Nemotron-4B 1,271 76.2 82.5 89.7 56.4\\nTable 6|Results on Retrieval Tasks.\\naccuracy of 49.6, surpassing Qwen3-1.7B-Base by 6.3 while being 47√ófaster. In contrast, prior linear attention\\nand hybrid models are far behind Qwen3 on math tasks.\\nResults on Commonsense Reasoning Tasks.Table 5 summarizes the results on commonsense reasoning\\ntasks. Qwen2.5 and Qwen3 are relatively weak in this domain. Nevertheless, Jet-Nemotron-2B, which uses\\nQwen2.5-1.5B as the starting point, still demonstrates strong results, achieving an average accuracy of 62.0,\\noutperforming all baseline models.\\nResults on Retrieval Tasks.Table 6 presents the results on retrieval tasks. Jet-Nemotron-2B outperforms\\nall baselines except Qwen3-1.7B-Base. When scaled to 4B, Jet-Nemotron-4B achieves the best average accuracy\\nof 76.2, while still maintaining 21√óspeedup compared to Qwen3.\\nResults on Coding Tasks.Table 7 shows the results on coding tasks. Jet-Nemotron-2B delivers a higher\\naverage accuracy than all baselines. Jet-Nemotron-4B achieves a higher accuracy across all coding tasks while\\nstill delivering a large advantage on generation throughput against leading LMs like Qwen3-1.7B-Base.\\nResults on Long-Context Tasks.A common concern with linear and hybrid architectures is their accuracy\\non long-context tasks. In Table 8, we evaluate this on LongBench [29] up to a 64K context length. Our findings\\n9'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 10,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nType Model Throughput Accuracy ‚Üë\\n(token/s) ‚Üë Avg. EvalPlus CRUXEval-I-cot CRUXEval-O-cot\\nQwen2.5-1.5B [4] 241 52.0 54.3 56.0 45.8\\nQwen3-1.7B-Base [5] 61 58.9 62.8 60.4 53.4\\nLlama3.2-3B [2] 60 44.0 35.5 54.7 41.7\\nMiniCPM-2B-128K [58] 18 34.2 40.7 29.9 31.9\\nùëÇ(ùëõ2)\\nSmollm2-1.7B [60] 32 36.2 20.6 49.5 38.6\\nMamba2-2.7B [50] 2,507 14.0 12.0 9.3 20.7\\nRWKV7-1.5B [10] 3,050 13.2 16.8 8.0 14.7ùëÇ(ùëõ)\\nRec.Gemma-2.6B [62] 2,355 36.8 29.5 46.7 34.2\\nGemma3n-E2B [73] 701 40.4 29.6 49.9 41.6\\nHymba-1.5B [44] 180 30.3 31.3 32.2 27.5\\nZamba2-1.2B [16] 71 20.1 12.7 21.1 26.4\\nJet-Nemotron-2B 2,885 59.5 60.8 61.1 56.7\\nHybrid\\nJet-Nemotron-4B 1,271 63.5 65.6 65.9 59.0\\nTable 7|Results on Coding Tasks.\\nType Model Throughput Accuracy ‚Üë\\n(token/s) ‚Üë Avg. Few-Shot Code Sum. Single-Doc Multi-Doc\\nQwen2.5-1.5B [4] 241 39.1 63.9 57.2 26.3 28.3 19.9\\nQwen3-1.7B-Base [5] 61 42.2 68.8 48.1 26.8 36.6 30.6\\nLlama3.2-3B [2] 60 39.9 65.2 58.0 24.3 27.6 24.6\\nMiniCPM-2B-128K [58] 18 41.1 57.3 59.6 25.7 33.4 29.6\\nùëÇ(ùëõ2)\\nSmollm2-1.7B [60] 32 21.3 38.9 28.6 16.0 13.2 9.8\\nMamba2-2.7B [50] 2,507 10.3 6.4 30.2 9.1 3.5 2.5\\nRWKV7-1.5B [10] 3,050 14.2 10.6 21.1 18.1 12.8 8.7ùëÇ(ùëõ)\\nRec.Gemma-2.6B [62] 2,355 24.1 31.8 56.7 12.9 9.2 9.6\\nGemma2-2.6B [73] 388 22.9 28.7 52.0 12.6 13.9 7.3\\nGemma3n-E2B [73] 701 40.4 56.4 67.2 25.6 29.3 28.6\\nHymba-1.5B [44] 180 28.0 36.1 53.5 51.8 14.0 19.8\\nZamba2-1.2B [16] 71 9.2 10.0 20.1 10.2 3.8 1.7\\nJet-Nemotron-2B 2,885 41.1 68.7 58.1 26.0 30.8 21.9\\nHybrid\\nJet-Nemotron-4B 1,271 43.9 69.7 63.2 26.4 32.5 27.5\\nTable 8|Results on Long-Context Tasks.\\nshow that Jet-Nemotron-2B, with two full-attention layers, achieves performance comparable to leading\\nmodels like Qwen2.5-1.5B and Gemma3n-E2B, which feature considerably more such layers. Furthermore, our\\nJet-Nemotron-4B outperforms Qwen3-1.7B-Base while delivering a 21√óspeedup in generation throughput.\\nThese results substantially advance the frontier of the efficiency-accuracy trade-off in long-context tasks.\\nSummary. Combined with previous results, Jet-Nemotron-2B and Jet-Nemotron-4B perform comparably\\nwith or even better than the advanced full-attention model (Qwen3-1.7B-Base) across all six evaluation\\ndomains. With significantly fewer full-attention layers and smaller KV cache size, Jet-Nemotron-2B and\\nJet-Nemotron-4B deliver 47√óand 21√óhigher generation throughput than Qwen3-1.7B-Base, respectively.\\n3.3. Efficiency Benchmark Results.\\nFigure 6 shows the throughput comparison between Qwen3-1.7B-Base and Jet-Nemotron-2B across various\\ncontext lengths. During the prefilling stage, Jet-Nemotron-2B is initially 1.14 and 1.15 times faster than\\nQwen3-1.7B-Base at shorter context lengths (4K and 8K). This can be further improved by designing a better\\noptimized kernel implementation of the JetBlock. As the context length increases, the benefits of linear\\n10'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 11,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\n0x\\n16x\\n32x\\n48x\\n64x\\n4K 8K 16K 32K 64K 128K 256K\\nQwen3-1.7B\\nJet-Nemotron-2B\\nRelative Decoding Speedup ‚Üë\\nContext Length\\n0x\\n2x\\n4x\\n6x\\n8x\\n4K 8K 16K 32K 64K 128K 256K\\nQwen3-1.7B\\nJet-Nemotron-2B\\nRelative PreÔ¨Ålling Speedup ‚Üë\\nContext Length\\n6.14\\n4.03\\n2.54\\n1.741.371.151.14\\n15.6\\n23.3\\n39.6\\n44.1 47.5 51.6 53.6Upper Bound: 56x\\nFigure 6|Efficiency Comparison Across Different Context Lengths.Jet-Nemotron-2B achieves up to\\na 6.14√óspeedup in prefilling and a 53.6√óspeedup in decoding compared to Qwen3-1.7B-Base.\\nattention become prominent, making Jet-Nemotron-2B achieve 6.14√óspeedup at a 256K context length.\\nDuring the decoding stage, Jet-Nemotron-2B consistently outperforms Qwen3-1.7B-Base by a large margin.\\nSince Jet-Nemotron-2B includes 2 full-attention layers with 2 groups of key-value states, its theoretical\\nmaximum speedup is14 √ó4 = 56times compared to Qwen3-1.7B-Base with 28 full-attention layers, where\\neach layer contains 8 groups of key-value states. In our throughput testbed, Jet-Nemotron-2B achieves a\\n15.6√óspeedup at a 4K context length and up to a 53.6√óspeedup at a 256K context length, almost reaching\\nthe theoretical upper bound.\\n4. Related Work\\nLarge language models (LLMs) are powerful but computationally intensive, motivating many works to build\\nefficient model architectures for LLMs. One line of research focuses on designing efficient linear attention\\nblocks [9, 10, 11, 12, 32, 50, 51, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 82, 84] or log-linear attention [85] blocks\\nto replace full attention blocks. Orthogonally, another line of research tries to combine full attention and linear\\nattention to build hybrid models [13, 15, 16, 17, 44, 86, 87, 88]. These works typically focus on the pre-training\\nsetting, and their accuracy lags behind leading full-attention models. Recently, there are some efforts on\\nlinearizing LLMs with full attention replaced with linear attention [89, 90, 91, 92, 93, 94, 95, 96]. However,\\ntheir model architecture are poorly optimized due to the large overhead of evaluating specific configuration,\\nand thus their results are still inferior to SOTA full-attention models.\\nOur work is also related to neural architecture search (NAS) [45, 97, 98, 99, 100], a powerful technique for\\nexploring the architectural design space and discovering novel model structures. In particular, hardware-aware\\nneural architecture search [45] enables the development of specialized model architectures optimized for target\\nhardware by training a once-for-all super-network [31], or leveraging layer-wise distillation [101, 102], etc.\\nHowever, NAS has been rarely applied in the era of large language models (LLMs) due to the prohibitive cost\\nof pretraining. Recent efforts have primarily focused on building flexible LLM architectures [103, 104], which\\ncan generate a range of subnetworks with varying depths and widths to accommodate different hardware\\nplatforms. Nevertheless, the architectural backbone of these subnetworks remains unchanged, relying entirely\\non full-attention layers.\\n5. Conclusion\\nWe introduce Jet-Nemotron, a new family of hybrid-architecture language models that outperform state-of-the-\\nart full-attention models ‚Äî including Qwen3, Qwen2.5, Gemma3, and Llama3.2 ‚Äî while delivering substantial\\nefficiency gains, with up to 53.6√óhigher generation throughput on H100 GPUs (256K context length,\\nmaximum batch size). Jet-Nemotron is enabled by two key innovations: (1) Post Neural Architecture Search,\\na highly efficient post-training architecture adaptation pipeline applicable to any pre-trained Transformer\\nmodel; and (2) the JetBlock, a novel linear attention block that significantly outperforms prior designs such\\nas Mamba2, GLA, and Gated DeltaNet. Extensive empirical results show that Jet-Nemotron achieves major\\nefficiency improvements without compromising accuracy across a broad range of benchmarks. Additionally,\\nJet-Nemotron significantly reduces the cost and risk associated with LLM architecture exploration, enabling\\nfaster and more efficient innovation in language model design.\\n11'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 12,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nReferences\\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:1877‚Äì1901, 2020.\\n[2] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\\nAiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models.arXiv preprint\\narXiv:2407.21783, 2024.\\n[3] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper,\\nBryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report.arXiv preprint\\narXiv:2406.11704, 2024.\\n[4] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei\\nHuang, Haoran Wei, et al. Qwen2. 5 technical report.arXiv preprint arXiv:2412.15115, 2024.\\n[5] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\\nHuang, Chenxu Lv, et al. Qwen3 technical report.arXiv preprint arXiv:2505.09388, 2025.\\n[6] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,\\nChenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report.arXiv preprint arXiv:2412.19437, 2024.\\n[7] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\\nSchalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal\\nmodels. arXiv preprint arXiv:2312.11805, 2023.\\n[8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017.\\n[9] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns: Fast\\nautoregressive transformers with linear attention. In International conference on machine learning, pages\\n5156‚Äì5165. PMLR, 2020.\\n[10] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu,\\nJanna Lu, William Merrill, et al. Rwkv-7\" goose\" with expressive dynamic state evolution.arXiv preprint\\narXiv:2503.14456, 2025.\\n[11] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers\\nwith hardware-efficient training.arXiv preprint arXiv:2312.06635, 2023.\\n[12] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\\nRetentive network: A successor to transformer for large language models.arXiv preprint arXiv:2307.08621, 2023.\\n[13] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo,\\nDa Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention.arXiv preprint\\narXiv:2501.08313, 2025.\\n[14] Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang,\\nCheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention.arXiv preprint\\narXiv:2506.13585, 2025.\\n[15] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang,\\nand Furu Wei. You only cache once: Decoder-decoder architectures for language models.Advances in Neural\\nInformation Processing Systems, 37:7339‚Äì7361, 2024.\\n[16] Paolo Glorioso, Quentin Anthony, Yury Tokpanov, Anna Golubeva, Vasudev Shyam, James Whittington,\\nJonathan Pilault, and Beren Millidge. The zamba2 suite: Technical report.arXiv preprint arXiv:2411.15242,\\n2024.\\n[17] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state\\nspace models for efficient unlimited context language modeling.arXiv preprint arXiv:2406.07522, 2024.\\n[18] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\nMeasuring massive multitask language understanding.arXiv preprint arXiv:2009.03300, 2020.\\n[19] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran\\nArulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding\\nbenchmark. InThe Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks\\nTrack, 2024.\\n[20] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian\\nMichael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. InFirst Conference on\\nLanguage Modeling, 2024.\\n12'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 13,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\n[21] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: To-\\nwards interpretable math word problem solving with operation-based formalisms.arXiv preprint arXiv:1905.13319,\\n2019.\\n[22] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems.arXiv\\npreprint arXiv:2110.14168, 2021.\\n[23] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and\\nChristopher R√©. Language models enable simple systems for generating structured views of heterogeneous data\\nlakes. Proc. VLDB Endow., 17(2):92‚Äì105, October 2023.\\n[24] Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. OpenCeres: When open information extraction meets\\nthe semi-structured web. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers), June 2019.\\n[25] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine\\ncomprehension of text. InProceedings of EMNLP, 2016.\\n[26] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\\nCarrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models.arXiv preprint\\narXiv:2108.07732, 2021.\\n[27] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,\\nDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William\\nSaunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec\\nRadford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on\\ncode, 2021.\\n[28] Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang.\\nCruxeval: A benchmark for code reasoning, understanding and execution. InForty-first International Conference\\non Machine Learning, 2024.\\n[29] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan\\nZeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual, multitask benchmark for\\nlong context understanding. InProceedings of the 62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 3119‚Äì3137, Bangkok, Thailand, August 2024. Association for\\nComputational Linguistics.\\n[30] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali\\nHatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models.\\narXiv preprint arXiv:2406.07887, 2024.\\n[31] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and\\nspecialize it for efficient deployment.arXiv preprint arXiv:1908.09791, 2019.\\n[32] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule.\\narXiv preprint arXiv:2412.06464, 2024.\\n[33] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\\nBoolQ: Exploring the surprising difficulty of natural yes/no questions. InProceedings of NAACL-HLT, 2019.\\n[34] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.arXiv:1803.05457v1,\\n2018.\\n[35] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in\\nnatural language. InProceedings of AAAI, 2020.\\n[36] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. InProceedings of KR,\\n2012.\\n[37] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), 2022.\\n13'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 14,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\n[38] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\\nnew dataset for open book question answering. InProceedings of EMNLP, 2018.\\n[39] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\\nSteinhardt. Measuring mathematical problem solving with the math dataset.arXiv preprint arXiv:2103.03874,\\n2021.\\n[40] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really\\ncorrect? rigorous evaluation of large language models for code generation. InThirty-seventh Conference on\\nNeural Information Processing Systems, 2023.\\n[41] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\\nTatiana Matejovicova, Alexandre Ram√©, Morgane Rivi√®re, et al. Gemma 3 technical report.arXiv preprint\\narXiv:2503.19786, 2025.\\n[42] Fnu Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov,\\nHannaneh Hajishirzi, Sham Kakade, Ali Farhadi, and Prateek Jain. Matformer: Nested transformer for elastic\\ninference. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and\\nResource Optimization (WANT@NeurIPS 2023), 2023.\\n[43] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable\\nimage recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pages\\n8697‚Äì8710, 2018.\\n[44] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang\\nLiu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. Hymba: A hybrid-head architecture for small\\nlanguage models. arXiv preprint arXiv:2411.13676, 2024.\\n[45] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware.\\narXiv preprint arXiv:1812.00332, 2018.\\n[46] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.arXiv preprint\\narXiv:1503.02531, 2015.\\n[47] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language models.\\nIn Proceedings of ICLR, 2024.\\n[48] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention\\ndistillation for task-agnostic compression of pre-trained transformers.Advances in neural information processing\\nsystems, 33:5776‚Äì5788, 2020.\\n[49] Alex Graves. Sequence transduction with recurrent neural networks. In Proceedings of the Workshop on\\nRepresentation Learning (ICML2012), 2012.\\n[50] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured\\nstate space duality.arXiv preprint arXiv:2405.21060, 2024.\\n[51] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the\\ndelta rule over sequence length.arXiv preprint arXiv:2406.06484, 2024.\\n[52] Jos Van Der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate.arXiv preprint\\narXiv:1804.04849, 2018.\\n[53] DL Prados and SC Kak. Neural network capacity using delta rule.Electronics Letters, 25(3):197‚Äì199, 1989.\\n[54] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight\\nand dynamic convolutions.arXiv preprint arXiv:1901.10430, 2019.\\n[55] Han Cai, Muyang Li, Qinsheng Zhang, Ming-Yu Liu, and Song Han. Condition-aware neural network for\\ncontrolled image generation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 7194‚Äì7203, 2024.\\n[56] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention for\\nhigh-resolution dense prediction. InProceedings of the IEEE/CVF international conference on computer vision,\\npages 17302‚Äì17313, 2023.\\n[57] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function\\napproximation in reinforcement learning.Neural networks, 107:3‚Äì11, 2018.\\n[58] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang,\\nWeilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies.\\narXiv preprint arXiv:2404.06395, 2024.\\n[59] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie\\nChang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing sub-billion parameter language\\nmodels for on-device use cases. InForty-first International Conference on Machine Learning, 2024.\\n14'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 15,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\n[60] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Mart√≠n Bl√°zquez, Guilherme Penedo, Lewis Tunstall,\\nAndr√©s Marafioti, Hynek Kydl√≠ƒçek, Agust√≠n Piqueres Lajar√≠n, Vaibhav Srivastav, et al. Smollm2: When smol\\ngoes big‚Äìdata-centric training of a small language model.arXiv preprint arXiv:2502.02737, 2025.\\n[61] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe\\nLu, Junjie Yan, et al. Muon is scalable for llm training.arXiv preprint arXiv:2502.16982, 2025.\\n[62] Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun,\\nLeonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, et al. Recurrentgemma: Moving past\\ntransformers for efficient open language models.arXiv preprint arXiv:2404.07839, 2024.\\n[63] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad\\nShoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into a refined long-horizon pretraining\\ndataset. arXiv preprint arXiv:2412.02595, 2024.\\n[64] Yaoyao Chang, Lei Cui, Li Dong, Shaohan Huang, Yangyu Huang, Yupan Huang, Scarlett Li, Tengchao Lv,\\nShuming Ma, Qinzheng Sun, et al. Redstone: Curating general, code, math, and qa data for large language\\nmodels. arXiv preprint arXiv:2412.03398, 2024.\\n[65] Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, and Eric P\\nXing. Megamath: Pushing the limits of open math corpora.arXiv preprint arXiv:2504.02807, 2025.\\n[66] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus,\\nJuly 2024.\\n[67] Kazuki Fujii, Yukito Tajima, Sakae Mizuki, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Masanari Ohi, Masaki\\nKawamura, Taishi Nakamura, Takumi Okamoto, et al. Rewriting pre-training data boosts llm performance in\\nmath and code.arXiv preprint arXiv:2505.02881, 2025.\\n[68] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\\nGolding, Jeffrey Hsu, Alain Le Noac‚Äôh, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason\\nPhang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben\\nWang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024.\\n[69] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. InThe Twelfth\\nInternational Conference on Learning Representations, 2024.\\n[70] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention\\nmechanism, January 2024.\\n[71] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, et al.\\nTransformers: State-of-the-art natural language processing. InProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System Demonstrations, 2020.\\n[72] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey\\nTumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in LLM inference with Sarathi-\\nServe. In18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 117‚Äì134,\\nSanta Clara, CA, July 2024. USENIX Association.\\n[73] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\\nL√©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram√©, et al. Gemma 2: Improving open\\nlanguage models at a practical size.arXiv preprint arXiv:2408.00118, 2024.\\n[74] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling.\\nAdvances in Neural Information Processing Systems, 36:33202‚Äì33221, 2023.\\n[75] Albert Gu, Karan Goel, and Christopher R√©. Efficiently modeling long sequences with structured state spaces.\\narXiv preprint arXiv:2111.00396, 2021.\\n[76] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. InInternational\\nconference on machine learning, pages 9099‚Äì9117. PMLR, 2022.\\n[77] Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, and Yiran Zhong. You only scan once:\\nEfficient multi-dimension sequential modeling with lightnet.arXiv preprint arXiv:2405.21022, 2024.\\n[78] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran\\nZhong. cosformer: Rethinking softmax in attention. InInternational Conference on Learning Representations,\\n2022.\\n[79] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. InInternational\\nConference on Learning Representations, 2020.\\n[80] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.arXiv\\npreprint arXiv:2009.14794, 2020.\\n15'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 16,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\n[81] Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin\\nWang, Wei Bi, Peng Zhou, and Guohong Fu. Gated slot attention for efficient linear-time sequence modeling. In\\nProceedings of NeurIPS, 2024.\\n[82] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong\\nWang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states.arXiv preprint\\narXiv:2407.04620, 2024.\\n[83] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T\\nFreeman, and Hao Tan. Test-time training done right.arXiv preprint arXiv:2505.23884, 2025.\\n[84] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time.arXiv preprint\\narXiv:2501.00663, 2024.\\n[85] Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear attention.arXiv\\npreprint arXiv:2506.04761, 2025.\\n[86] Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek,\\nAlexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, et al. Nemotron-h:\\nA family of accurate and efficient hybrid mamba-transformer models.arXiv preprint arXiv:2504.03624, 2025.\\n[87] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked\\nMeirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language model.\\narXiv preprint arXiv:2403.19887, 2024.\\n[88] Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. Mom: Linear sequence modeling with mixture-of-\\nmemories. arXiv preprint arXiv:2502.13685, 2025.\\n[89] Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling\\nand accelerating hybrid models.Advances in Neural Information Processing Systems, 37:62432‚Äì62457, 2024.\\n[90] Michael Zhang, Simran Arora, Rahul Chalamala, Benjamin Frederick Spector, Alan Wu, Krithik Ramesh, Aaryan\\nSinghal, and Christopher Re. Lolcats: On low-rank linearizing of large language models. InThe Thirteenth\\nInternational Conference on Learning Representations, 2024.\\n[91] Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, and Yu Cheng. Liger: Linearizing large language models to gated\\nrecurrent structures. InInternational conference on machine learning, 2025.\\n[92] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen,\\nand Noah A Smith. Finetuning pretrained transformers into rnns. InProceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing, pages 10630‚Äì10643, 2021.\\n[93] Aviv Bick, Kevin Li, Eric P. Xing, J Zico Kolter, and Albert Gu. Transformers to SSMs: Distilling quadratic\\nknowledge to subquadratic models. InThe Thirty-eighth Annual Conference on Neural Information Processing\\nSystems, 2024.\\n[94] Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar.\\nLinearizing large language models.arXiv preprint arXiv:2405.06640, 2024.\\n[95] Hanting Chen, Liu Zhicheng, Xutao Wang, Yuchuan Tian, and Yunhe Wang. Dijiang: Efficient large language\\nmodels through compact kernelization. InInternational Conference on Machine Learning, pages 7103‚Äì7117.\\nPMLR, 2024.\\n[96] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine:\\nExpressive linear attentions with softmax mimicry. In The Twelfth International Conference on Learning\\nRepresentations, 2024.\\n[97] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\\narXiv:1611.01578, 2016.\\n[98] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network\\ntransformation. In Proceedings of the AAAI conference on artificial intelligence, 2018.\\n[99] Xuan Shen, Pu Zhao, Yifan Gong, Zhenglun Kong, Zheng Zhan, Yushu Wu, Ming Lin, Chao Wu, Xue Lin, and\\nYanzhi Wang. Search for efficient large language models.Advances in Neural Information Processing Systems,\\n37:139294‚Äì139315, 2024.\\n[100] Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, and Jun Wang. Merino: Entropy-driven design for\\ngenerative language models on iot devices. InProceedings of the AAAI Conference on Artificial Intelligence,\\npages 22840‚Äì22848, 2025.\\n[101] Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammed Dabbah, Ido Galil,\\nAmnon Geifman, Yonatan Geifman, Izhak Golan, et al. Puzzle: Distillation-based nas for inference-optimized\\nllms. In Forty-second International Conference on Machine Learning, 2025.\\n16'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 17,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\n[102] Pavlo Molchanov, Jimmy Hall, Hongxu Yin, Jan Kautz, Nicolo Fusi, and Arash Vahdat. Lana: latency aware\\nnetwork acceleration. InEuropean Conference on Computer Vision, pages 137‚Äì156. Springer, 2022.\\n[103] Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, and Pavlo Molchanov.\\nFlextron: Many-in-one flexible large language model.arXiv preprint arXiv:2406.10260, 2024.\\n[104] Ruisi Cai, Saurav Muralidharan, Hongxu Yin, Zhangyang Wang, Jan Kautz, and Pavlo Molchanov. Llamaflex:\\nMany-in-one llms via generalized pruning and weight sharing. InThe Thirteenth International Conference on\\nLearning Representations, 2025.\\n[105] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa:\\nTraining generalized multi-query transformer models from multi-head checkpoints. InProceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing, pages 4895‚Äì4901, 2023.\\n17'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 18,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nA. Experimental Details\\nA.1. Final Model Architecture\\nThe final Jet-Nemotron models are composed of a stack of blocks, each containing a Multi-Layer Perceptron\\n(MLP) layer and an attention layer. The attention layer is selected from one of three types: full attention,\\nsliding window attention, or JetBlock. The detailed architecture configurations are presented in Table 9.\\nJet-Nemotron-2B Jet-Nemotron-4B\\nTotal blocks 28 36\\nFull Attention Layers No. 15, 20 No. 18, 21, 22, 28, 33\\nSliding Window Attention Layers No. 21, 22 No. 17, 20, 23, 24, 26\\nVocabulary Size 151,643 151,643\\nHidden Size 1,536 2,048\\nMLP Intermediate Size 8,960 11,008\\nTable 9|The overall model architectures of Jet-Nemotron families.\\nThe full attention and sliding window attention layers use grouped-query attention [105] and are configured\\nas in Table 10. For sliding window attention layers, the window size is set to 1,152 in Jet-Nemotron-2B and\\n2,048 in Jet-Nemotron-4B.\\nFull Attention / SWA Jet-Nemotron-2B Jet-Nemotron-4B\\nAttention Head Number 12 16\\nDimensions of Q/K/V 128 128\\nK/V Head Number 2 2\\nPosition Embedding RoPE RoPE\\nTable 10|The configurations of full-attention layers in Jet-Nemotron models.\\nThe configuration of JetBlock are shown in Table 11 :\\nJetBlock Jet-Nemotron-2B Jet-Nemotron-4B\\nQ/K Dimension 96 128\\nV Dimension 256 256\\nHead Number 12 16\\nConvolution Kernel Size 4 4\\nDConv Generator Hidden Size 32 32\\nTable 11|The configurations of JetBlock.\\nA.2. Experimental Costs\\nTable 12 summarizes the costs for PostNAS and training the Jet-Nemotron-2B model. We used 32 H100\\nGPUs in parallel. The reported GPU hours already account for the total number of devices.\\nA.3. Throughput Measurement\\nThroughout the experiments, we measure the maximum reachable prefilling and decoding throughput of\\nJet-Nemotron and the baselines on a single H100 GPU. This is achieved by adjusting the chunk size in\\nchunk-prefilling [72] to maximize the decoding batch size without sacrificing the prefilling throughput. We list\\nthe optimized batch size and the corresponding chunk size for each model in Table 13. The prefilling context\\nlength is 64K. Since the KV cache memory dominates GPU usage during inference, by reducing the memory\\nfootprint per sequence, smaller caches allow more sequences to be processed in parallel, greatly boosting\\ngeneration throughput.\\n18'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 19,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nTokens (B) ZFLOPs Time (H100 GPU Hours)\\nPostNAS\\nFull Attention Placement and Elimination 50 0.8 808\\nLinear Attention Block Selection 50 4.0 3120\\nNew Attention Block Design 50 0.8 624\\nHarware-Aware Arch Search 50 7.2 5616\\nTraining Stage1 50 0.8 624\\nStage2 350 5.6 7536\\nTable 12|Experimental Costs for PostNAS and training the Jet-Nemotron-2B model.\\nModel Batch Size Chunk Size\\nQwen2.5-1.5B 32 4,096\\nQwen3-1.7B 8 4,096\\nLlama3.2-1B 32 4,096\\nMiniCPM-2B-128K 2 2,048\\nPythia-2.8B 2 16,384\\nSmollm2-1.7B 4 16,384\\nMamba2-2.7B 128 1,024\\nRWKV7-1.5B 256 2,048\\nRec.Gemma-2B 128 512\\nGemma3n-E2B 64 4,096\\nGemma2-2.6B 16 2,048\\nHymba-1.5B 64 512\\nZamba2-1.2B 8 8,192\\nJet-Nemotron-2B 128 1,024\\nJet-Nemotron-4B 64 512\\nTable 13|Hyper-Parameters in Efficiency Measurement. We adjust the chunk size to maximize\\ndecoding batch size without compromising prefilling throughput.\\nB. Additional Results\\nB.1. Controlled Study on Training Data\\nTo exclude the influence of training data, we continually pre-train the baseline models (Qwen2.5, RWKV-7,\\nand Mamba-2) on Jet-Nemotron ‚Äôs training dataset to provide a more comprehensive evaluation. The results\\nin Table 14 show that Jet-Nemotron-2B outperforms all these finetuned baseline models by a significant\\nmargin.\\nModel MMLU Math Commonsense Retrieval\\nQwem2.5-1.5B-continual 56.7 37.6 59.8 71.5\\nMamba2-2.7B-continual 41.0 22.5 56.9 55.9\\nRWKV7-1.5B-continual 49.8 25.2 59.3 57.2\\nJet-Nemotron-2B 59.6 40.2 61.7 73.6\\nTable 14|Controlled Study on Training Data. All models are pre-trained or continually pre-trained on\\nthe Jet-Nemotron stage-2 training corpus discussed in Section 3.1.\\nB.2. Throughput Results on Lower-End Hardware\\nWe measure the throughput of Jet-Nemotron-2B and Qwen2.5-1.5B on the NVIDIA Jetson Orin (32GB) and\\nNVIDIA RTX 3090 GPUs with a context length of 64K. Results in Table 15 show that Jet-Nemotron-2B\\nachieves 8.84√óand 6.50√óspeedups over Qwen2.5-1.5B on the Jetson Orin and RTX 3090 GPUs, respectively.\\n19'},\n",
       " {'pdf_name': 'Jet_Nemotron_Eifficient_LM_with_Post_NAS_1756652780.pdf',\n",
       "  'page_number': 20,\n",
       "  'text': 'Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search\\nHardware Qwen2.5-1.5B (Tokens/s) Jet-Nemotron-2B (Tokens/s) SpeedUp\\nOrin 6.22 55.00 8.84\\n3090 105.18 684.01 6.50\\nTable 15|Throughput Results on Jetson Orin (32GB) and NVIDIA RTX 3090 GPUs.\\n20'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = doc_loader.find_pdfs(\"/Users/swastikmishra/Downloads/tmp\")\n",
    "all_text = []\n",
    "for i in files:\n",
    "    text = doc_loader.extract_pdf_text(i)\n",
    "    for j in text:\n",
    "        all_text.append(j)\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a496045",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = vector.embed([i[\"text\"] for i in all_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ac6a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035efc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, tmp_dir = vector.create_db(all_text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5389d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "from fastembed import TextEmbedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b397dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(query_string, path):\n",
    "    index = faiss.read_index(os.path.join(path, \"index.faiss\"))\n",
    "    with open(os.path.join(path, \"metadata.pkl\"), \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    embedding_model = TextEmbedding(model_name = \"BAAI/bge-small-en-v1.5\")\n",
    "    query_vector = np.array(embedding_model.embed([query_string])).reshape(1, -1)\n",
    "    print(index.search(query_vector, 5))\n",
    "    distances, indices = index.search(query_vector, 5)\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        meta = metadata[idx]\n",
    "        print(f\"Score: {score}, PDF: {meta['pdf_name']}, Page: {meta['page_number']}\")\n",
    "        print(meta[\"text\"][:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d34d05",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mquery_db\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory-bandwidth-bound rather than compute-bound\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mquery_db\u001b[39m\u001b[34m(query_string, path)\u001b[39m\n\u001b[32m      5\u001b[39m embedding_model = TextEmbedding(model_name = \u001b[33m\"\u001b[39m\u001b[33mBAAI/bge-small-en-v1.5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m query_vector = np.array(embedding_model.embed([query_string])).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      8\u001b[39m distances, indices = index.search(query_vector, \u001b[32m5\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m score, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(distances[\u001b[32m0\u001b[39m], indices[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/SnapIndex/.venv/lib/python3.13/site-packages/faiss/class_wrappers.py:348\u001b[39m, in \u001b[36mhandle_Index.<locals>.replacement_search\u001b[39m\u001b[34m(self, x, k, params, D, I, numeric_type)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[32m    322\u001b[39m \n\u001b[32m    323\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    344\u001b[39m \u001b[33;03m    When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    347\u001b[39m n, d = x.shape\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_numeric_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m d == \u001b[38;5;28mself\u001b[39m.d\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m k > \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: float() argument must be a string or a real number, not 'generator'"
     ]
    }
   ],
   "source": [
    "query_db(\"memory-bandwidth-bound rather than compute-bound\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9a36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
